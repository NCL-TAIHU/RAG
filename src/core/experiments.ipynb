{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16043449",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import LLM, SamplingParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d83594",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = LLM(model = \"meta-llama/Llama-3.1-8B-Instruct\")\n",
    "sampling = SamplingParams(temperature=0.7, max_tokens=100)\n",
    "response = llm.generate(\"Explain quantum entanglement in simple terms.\", sampling)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "768f31dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 30 files: 100%|██████████| 30/30 [00:00<00:00, 96866.14it/s]\n",
      "initial target device: 100%|██████████| 3/3 [00:12<00:00,  4.25s/it]\n",
      "Chunks:   0%|          | 0/2 [00:00<?, ?it/s]You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Chunks: 100%|██████████| 2/2 [00:00<00:00,  2.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dense_vecs': None, 'lexical_weights': [defaultdict(<class 'int'>, {'4865': np.float16(0.1172), '83': np.float16(0.0817), '70': np.float16(0.09796), '10323': np.float16(0.2803), '111': np.float16(0.118), '9942': np.float16(0.292), '32': np.float16(0.0554)}), defaultdict(<class 'int'>, {'60075': np.float16(0.1252), '16442': np.float16(0.1963), '70': np.float16(0.05466), '154453': np.float16(0.216), '111': np.float16(0.05447), '90816': np.float16(0.268), '939': np.float16(0.0979), '5': np.float16(0.0864)})], 'colbert_vecs': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from FlagEmbedding import BGEM3FlagModel\n",
    "\n",
    "model = BGEM3FlagModel(\"BAAI/bge-m3\", use_fp16=True)\n",
    "sample_texts = [\"What is the capital of France?\", \"Explain the theory of relativity.\"]\n",
    "output = model.encode(sample_texts,  return_dense=False,\n",
    "            return_sparse=True,\n",
    "            return_colbert_vecs=False)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c5e4df9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.xlm_roberta.tokenization_xlm_roberta_fast.XLMRobertaTokenizerFast'>\n"
     ]
    }
   ],
   "source": [
    "print(type(model.tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34a0dcc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250002\n"
     ]
    }
   ],
   "source": [
    "print(model.tokenizer.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e17fbc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
