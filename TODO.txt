1. abstract is confusing, should be abstracts
2. Prompt generation should be one class
3. Embedding functions should be umbrellaed under an interface
4. should have document class, indexable by ID, which gets metadata and abstract, instead of metadata and abstract two separate lists
5. Should be a data postprocessor
6. LLM should be independent class, wrapping a manager around it makes unecassary clunkiness. 
7. Test different embedding functions and filtering abilities. 
8. Create command line / jupyter application to test on different query configurations (filtering, prompt etc.). 
9. Write embedding question generator (LLM class, prompt generator). 

LLM Builder
Database Builder
Prompt Builder
load data 
embedder, embedding function and output configurations 
Query can be an object, including metadata filter. 
--> Is dense and sparse embedding separately stored? 

Post Processor


#parse keywords



分工工作模式
程式碼分工方式
git commit


ElasticSearch is not a filter in a functional sense. 
It's a stateful object that needs to be maintained and synchronized with the vector db. 
The filtered elements is a function of the database state and the query, not just the query. 

Therefore, we need something to track "a thing that can be maintained by insertion of data" 
That's called a database
But abstracting a "database" is too abstract, because each database requires drastically different preprocessing techniques and 
data formats transformation.
call it a subset, not a filter. 

extract embed abstracts to be a functionality of vector db manager


at the manager level, we want to include routing capability, thus we should delegate hybrid-orchestration to search engine level

In building this system, I’ve designed for adjustability — the architecture exposes many degrees of freedom across routing logic, 
embedding strategies, and filtering behavior. But without visibility, adjustability is blind. 
That’s where monitoring comes in: it serves as the system’s vision, enabling it to observe itself in action. 
With vision and mobility combined, the system can begin to learn — not just respond to queries, but actively navigate the space of search strategies to find the sweet spots. This pairing transforms static retrieval into a dynamic, self-correcting process — one that can adapt over time toward better performance, interpretability, and intent alignment.

Adjust knobs based on info
Knobs:
Hybridization alpha
Language mixture weights

Info: 
Metadata filters
Monitored performance distribution for each subset
There can be different distribution for each characteristic, and each filter would belong to multiple such subsets
We can make the decision based on the combined information of these different subset partitions, and such weight 
can be based on the confidence that subset is a meaningful indicator (If a partition, like school name, doesn't affect search method accuracy, then
the results should be pretty even across schools. But with something like domain, it may vary a lot. However it's challenging to determine whether that is innate variance
caused by randomness or true indication of meaningfulness.)

To persistently store vectors, we created the vector store objects. But that alone isn’t enough—how does a search engine know when to embed a new vector on-the-fly versus when to retrieve a precomputed one? That’s where the vector manager comes in. Given a list of documents, the vector manager decides which ones already have stored embeddings and which ones need to be embedded, handling both retrieval and computation seamlessly.

The vector store itself maintains an in-memory cache while the system is running. When a new document is inserted, its embedding is stored in memory so that it can be reused later—potentially by a different search engine—without recomputing. However, runtime updates don’t immediately affect the file system. We reserve persistent updates for explicit calls to .save().

This design is intentional. By separating in-memory caching from permanent storage, we allow the system to remain fast and flexible during operation, while keeping storage updates standardized and version-controlled. The precompute.py script is responsible for bulk updates—it precomputes embeddings and writes them to disk in one go. This ensures reproducibility and avoids polluting the vector store with untracked or inconsistent additions.

Factory is the control center. 
Core problem: How do we assign configurations to specific search engines without overcomplicating the search api? 
Solution: In factory, we tell all the search engines their way of fulfilling the search requests (including alphas and other hyperparameters), 
So that when the system is running, the search request can be simple, under the factory-constructed environment. 

How do we control the content channel for each document?
Make the representative content of each document a variable, that can be dynamically stored. 
If we were to allow one document to have multiple content channels. 

1. Does each document need to have the same amount of channels? (If we do title as channel, maybe not, but we may want to support future chunking.)
2. Are cross-channel vectors put into the same collection? (idea: Same model, Same collection. But we need to handle primary key duplication problem.)
model/
    dataset/
        <channel>.json: 
            {id: str, (<chunk id>: <vector>)*}

** chunk size can be a metadata in the vector store** 

Channel and chunks should two separate concepts, because it makes sense to insert multiple chunks
into the same vector collection, and each document can have multiple chunks. 

Each channel should be one file, which allows one vector store to mount on it. Each vector store can 
be seen as a meaningful unit of vector collection. 

<channel, chunk number> determines a specific embedding for a document. 

Search engine should still return unit as document. But it can use vector stores with chunk embeddings of different chunk sizes. 
The user can control the versioning and chunk size via metadata and force rebuild. 
Chunk handling should be handled internally by the search engine, according to it's specifications. 


Okay great. Recall that in our document abstraction, we defined multiple channels that outputs semantic texts that represents a document. In previous versions, we simply used the first channel. Now, we want to introduce some more flexibility and concepts: configurable channeling and chunking. Specifically, each channel of each document can have multiple chunks, and since it makes sense to put multiple chunks of the same channel into the same vector collection, we want each vector store to be a channel of specific dataset embedded. Specifically, the vector store data storage should be like this: 
model/
    dataset/
        <channel>.json: 
            {id: str, (<chunk id>: <vector>)*}. 
Now, we want to change several things. In essence we want to change the contract between search engine and vector managers, and hence vector stores. Because now each document (channel set) can have multiple chunks, the manager should return {(<doc_id>: {<chunk id>: vector}, )*} instead of  List[List[float]], and so should the vector store (This aligns with its internal data storage). When the search engine receives it, it concatenates the nested index information into the primary key that corresponds to a vector, and inserts it in the database. It then does the grouping and fulfills the search engine contract of returning a list of documents rather than a list of chunks. To summarize, we want to: 
1. The vector store data storage and output format. 
2. Change the vector manager format. 
3. Change insert and add grouping for search engine. 
We would do it step by step, and I would give you the original code to change, don't come up with random things. I want you to understand the instructions first. Is this clear enough?  

The vector manager should handle the channeling, namely, what strings to pass to the chunker. It can retain the channel name as an attribute and assert its validity by comparing the document type content schema. 
No, I told you that the chunker should be purely utility class that chunks strings, like this: 




Larger system thoughts: 
I have some deeper and more thorough thoughts on the design pattern, I would articulate them to you. We think of our current components and infer what they should do using the Model-View-Controller (MVC) design pattern. The way the three components interact with each other is as follows: The model is the current state of the system, the view read-access it, the controller can read-write model, responding to the view's request for it to do so. In our case, the view is the management-ui, the controller is the backend. The Model is less straight forward. In principle, it is the state in which the system is at. However, the "state" can refer to the static file system state or the dynamic in-memory state, for example, the record of which apps are created can be stored as json files, but the SearchApp objects that actively interacts with requests and retrieves data from Milvus resides in memory. Using this design pattern, we can be consistent and avoid plausible but diverging anti patterns, for example if we let the backend read the App config files and returning them to the front end for display, we are forcing the view to read the state via the controller, which the view should directly read the state. Under this framework, we can have a "model" state which is a process that is aware of all current created apps but not yet activate. It can then respond to the backend's request to activate certain apps, or initiate it, those apps would be present in the memory of that thread. When the model process is down, neither the frontend nor backend would function. When only the backend is down, the frontend can display the current active apps but it cannot do any write access to them. When only frontend is down, programmers can send custom requests to the backend and make write access to the model. What I'm a bit not sure is how the main function of the model thread would be like, maybe just read in the file data and maintain a list of active apps as a global variable and have listeners or async functions? What do you think about my design thoughts? 

Vector Store should be independent from search engine. 


multi user
end to end response time
resource, machine spec. 