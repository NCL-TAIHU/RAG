1. abstract is confusing, should be abstracts
2. Prompt generation should be one class
3. Embedding functions should be umbrellaed under an interface
4. should have document class, indexable by ID, which gets metadata and abstract, instead of metadata and abstract two separate lists
5. Should be a data postprocessor
6. LLM should be independent class, wrapping a manager around it makes unecassary clunkiness. 
7. Test different embedding functions and filtering abilities. 
8. Create command line / jupyter application to test on different query configurations (filtering, prompt etc.). 
9. Write embedding question generator (LLM class, prompt generator). 

LLM Builder
Database Builder
Prompt Builder
load data 
embedder, embedding function and output configurations 
Query can be an object, including metadata filter. 
--> Is dense and sparse embedding separately stored? 

Post Processor


#parse keywords



分工工作模式
程式碼分工方式
git commit


ElasticSearch is not a filter in a functional sense. 
It's a stateful object that needs to be maintained and synchronized with the vector db. 
The filtered elements is a function of the database state and the query, not just the query. 

Therefore, we need something to track "a thing that can be maintained by insertion of data" 
That's called a database
But abstracting a "database" is too abstract, because each database requires drastically different preprocessing techniques and 
data formats transformation.
call it a subset, not a filter. 

extract embed abstracts to be a functionality of vector db manager


at the manager level, we want to include routing capability, thus we should delegate hybrid-orchestration to search engine level

In building this system, I’ve designed for adjustability — the architecture exposes many degrees of freedom across routing logic, 
embedding strategies, and filtering behavior. But without visibility, adjustability is blind. 
That’s where monitoring comes in: it serves as the system’s vision, enabling it to observe itself in action. 
With vision and mobility combined, the system can begin to learn — not just respond to queries, but actively navigate the space of search strategies to find the sweet spots. This pairing transforms static retrieval into a dynamic, self-correcting process — one that can adapt over time toward better performance, interpretability, and intent alignment.

Adjust knobs based on info
Knobs:
Hybridization alpha
Language mixture weights

Info: 
Metadata filters
Monitored performance distribution for each subset
There can be different distribution for each characteristic, and each filter would belong to multiple such subsets
We can make the decision based on the combined information of these different subset partitions, and such weight 
can be based on the confidence that subset is a meaningful indicator (If a partition, like school name, doesn't affect search method accuracy, then
the results should be pretty even across schools. But with something like domain, it may vary a lot. However it's challenging to determine whether that is innate variance
caused by randomness or true indication of meaningfulness.)

To persistently store vectors, we created the vector store objects. But that alone isn’t enough—how does a search engine know when to embed a new vector on-the-fly versus when to retrieve a precomputed one? That’s where the vector manager comes in. Given a list of documents, the vector manager decides which ones already have stored embeddings and which ones need to be embedded, handling both retrieval and computation seamlessly.

The vector store itself maintains an in-memory cache while the system is running. When a new document is inserted, its embedding is stored in memory so that it can be reused later—potentially by a different search engine—without recomputing. However, runtime updates don’t immediately affect the file system. We reserve persistent updates for explicit calls to .save().

This design is intentional. By separating in-memory caching from permanent storage, we allow the system to remain fast and flexible during operation, while keeping storage updates standardized and version-controlled. The precompute.py script is responsible for bulk updates—it precomputes embeddings and writes them to disk in one go. This ensures reproducibility and avoids polluting the vector store with untracked or inconsistent additions.

Factory is the control center. 
Core problem: How do we assign configurations to specific search engines without overcomplicating the search api? 
Solution: In factory, we tell all the search engines their way of fulfilling the search requests (including alphas and other hyperparameters), 
So that when the system is running, the search request can be simple, under the factory-constructed environment. 
