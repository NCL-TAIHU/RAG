1. abstract is confusing, should be abstracts
2. Prompt generation should be one class
3. Embedding functions should be umbrellaed under an interface
4. should have document class, indexable by ID, which gets metadata and abstract, instead of metadata and abstract two separate lists
5. Should be a data postprocessor
6. LLM should be independent class, wrapping a manager around it makes unecassary clunkiness. 
7. Test different embedding functions and filtering abilities. 
8. Create command line / jupyter application to test on different query configurations (filtering, prompt etc.). 
9. Write embedding question generator (LLM class, prompt generator). 

LLM Builder
Database Builder
Prompt Builder
load data 
embedder, embedding function and output configurations 
Query can be an object, including metadata filter. 
--> Is dense and sparse embedding separately stored? 

Post Processor


#parse keywords



分工工作模式
程式碼分工方式
git commit


ElasticSearch is not a filter in a functional sense. 
It's a stateful object that needs to be maintained and synchronized with the vector db. 
The filtered elements is a function of the database state and the query, not just the query. 

Therefore, we need something to track "a thing that can be maintained by insertion of data" 
That's called a database
But abstracting a "database" is too abstract, because each database requires drastically different preprocessing techniques and 
data formats transformation.
call it a subset, not a filter. 

extract embed abstracts to be a functionality of vector db manager


at the manager level, we want to include routing capability, thus we should delegate hybrid-orchestration to search engine level

In building this system, I’ve designed for adjustability — the architecture exposes many degrees of freedom across routing logic, 
embedding strategies, and filtering behavior. But without visibility, adjustability is blind. 
That’s where monitoring comes in: it serves as the system’s vision, enabling it to observe itself in action. 
With vision and mobility combined, the system can begin to learn — not just respond to queries, but actively navigate the space of search strategies to find the sweet spots. This pairing transforms static retrieval into a dynamic, self-correcting process — one that can adapt over time toward better performance, interpretability, and intent alignment.

Adjust knobs based on info
Knobs:
Hybridization alpha
Language mixture weights

Info: 
Metadata filters
Monitored performance distribution for each subset
There can be different distribution for each characteristic, and each filter would belong to multiple such subsets
We can make the decision based on the combined information of these different subset partitions, and such weight 
can be based on the confidence that subset is a meaningful indicator (If a partition, like school name, doesn't affect search method accuracy, then
the results should be pretty even across schools. But with something like domain, it may vary a lot. However it's challenging to determine whether that is innate variance
caused by randomness or true indication of meaningfulness.)

To persistently store vectors, we created the vector store objects. But that alone isn’t enough—how does a search engine know when to embed a new vector on-the-fly versus when to retrieve a precomputed one? That’s where the vector manager comes in. Given a list of documents, the vector manager decides which ones already have stored embeddings and which ones need to be embedded, handling both retrieval and computation seamlessly.

The vector store itself maintains an in-memory cache while the system is running. When a new document is inserted, its embedding is stored in memory so that it can be reused later—potentially by a different search engine—without recomputing. However, runtime updates don’t immediately affect the file system. We reserve persistent updates for explicit calls to .save().

This design is intentional. By separating in-memory caching from permanent storage, we allow the system to remain fast and flexible during operation, while keeping storage updates standardized and version-controlled. The precompute.py script is responsible for bulk updates—it precomputes embeddings and writes them to disk in one go. This ensures reproducibility and avoids polluting the vector store with untracked or inconsistent additions.

Factory is the control center. 
Core problem: How do we assign configurations to specific search engines without overcomplicating the search api? 
Solution: In factory, we tell all the search engines their way of fulfilling the search requests (including alphas and other hyperparameters), 
So that when the system is running, the search request can be simple, under the factory-constructed environment. 

How do we control the content channel for each document?
Make the representative content of each document a variable, that can be dynamically stored. 
If we were to allow one document to have multiple content channels. 

1. Does each document need to have the same amount of channels? (If we do title as channel, maybe not, but we may want to support future chunking.)
2. Are cross-channel vectors put into the same collection? (idea: Same model, Same collection. But we need to handle primary key duplication problem.)
model/
    dataset/
        <channel>.json: 
            {id: str, (<chunk id>: <vector>)*}

** chunk size can be a metadata in the vector store** 

Channel and chunks should two separate concepts, because it makes sense to insert multiple chunks
into the same vector collection, and each document can have multiple chunks. 

Each channel should be one file, which allows one vector store to mount on it. Each vector store can 
be seen as a meaningful unit of vector collection. 

<channel, chunk number> determines a specific embedding for a document. 

Search engine should still return unit as document. But it can use vector stores with chunk embeddings of different chunk sizes. 
The user can control the versioning and chunk size via metadata and force rebuild. 
Chunk handling should be handled internally by the search engine, according to it's specifications. 


Okay great. Recall that in our document abstraction, we defined multiple channels that outputs semantic texts that represents a document. In previous versions, we simply used the first channel. Now, we want to introduce some more flexibility and concepts: configurable channeling and chunking. Specifically, each channel of each document can have multiple chunks, and since it makes sense to put multiple chunks of the same channel into the same vector collection, we want each vector store to be a channel of specific dataset embedded. Specifically, the vector store data storage should be like this: 
model/
    dataset/
        <channel>.json: 
            {id: str, (<chunk id>: <vector>)*}. 
Now, we want to change several things. In essence we want to change the contract between search engine and vector managers, and hence vector stores. Because now each document (channel set) can have multiple chunks, the manager should return {(<doc_id>: {<chunk id>: vector}, )*} instead of  List[List[float]], and so should the vector store (This aligns with its internal data storage). When the search engine receives it, it concatenates the nested index information into the primary key that corresponds to a vector, and inserts it in the database. It then does the grouping and fulfills the search engine contract of returning a list of documents rather than a list of chunks. To summarize, we want to: 
1. The vector store data storage and output format. 
2. Change the vector manager format. 
3. Change insert and add grouping for search engine. 
We would do it step by step, and I would give you the original code to change, don't come up with random things. I want you to understand the instructions first. Is this clear enough?  

The vector manager should handle the channeling, namely, what strings to pass to the chunker. It can retain the channel name as an attribute and assert its validity by comparing the document type content schema. 
No, I told you that the chunker should be purely utility class that chunks strings, like this: 